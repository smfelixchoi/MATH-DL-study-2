{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let $A=[a_{ij}] \\in \\mathbb{R}^{n\\times n}$ be a square matrix and define $f \\colon \\mathbb{R}^n \\to \\mathbb{R}$ by $f(\\bm{x}) = \\bm{x}A\\bm{x}^\\top$. Find the gradient of $f$, that is,\n",
    "$$\n",
    "\\nabla f = \\left[ \\frac{\\partial f}{\\partial x_1} \\;\\; \\frac{\\partial f}{\\partial x_2} \\;\\; \\cdots \\;\\; \\frac{\\partial f}{\\partial x_n}\\right].\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider a classification problem with $K$ classes. We use 2-layer Multi-layer Perceptron to classify the dataset. Derive the gradients with respect to the weights (and biases) when the activation function of the hidden layer is ReLU.\n",
    "    \n",
    "    - Hint: We use the chain rule. Hence the one and only difference from using sigmoid is $\\displaystyle \\frac{\\partial \\bm{a}^n}{\\partial \\bm{z}^{n,1}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Complete the code below. The code describes a way to train 2-layer MLP with ReLU that classifies MNIST dataset.\n",
    "\n",
    "    - ReLU Function $\\to$ He Normal/Uniform\n",
    "$$ w_{ij} \\sim N\\left(0, \\sqrt{\\frac{2}{n_{in}}}^2 \\right) \\quad \\text{or} \\quad w_{ij} \\sim \\mathrm{Unif}\\left(-\\sqrt{\\frac{6}{n_{in}}}, \\;\\sqrt{\\frac{6}{n_{in}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "# load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Data split \n",
    "x_train, x_valid = x_train[:50000], x_train[50000:]\n",
    "y_train, y_valid = y_train[:50000], y_train[50000:]\n",
    "\n",
    "# Flatten data into row vectors and normalize into [0,1]\n",
    "x_train = x_train.reshape(50000,28*28)/256\n",
    "x_valid = x_valid.reshape(-1, 28*28)/256\n",
    "x_test  = x_test.reshape(-1, 28*28)/256\n",
    "\n",
    "y_train_one_hot = np.eye(10)[y_train]\n",
    "\n",
    "# dimensions\n",
    "INPUT_DIM=28*28\n",
    "HIDDEN_DIM=100\n",
    "OUTPUT_DIM=10\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1.+ np.exp(-z))\n",
    "\n",
    "######################################################################################\n",
    "############################## COMPLETE THE FUNCTION #################################\n",
    "def relu(z):\n",
    "    return \n",
    "######################################################################################\n",
    "\n",
    "def softmax(Z):\n",
    "    maxes = np.max(Z, axis=1, keepdims=True)\n",
    "    e = np.exp(Z-maxes)\n",
    "    p = e / np.sum(e, axis=1, keepdims=True)\n",
    "    return p\n",
    "\n",
    "# Forward pass\n",
    "def forward_NN(X, params, activation):\n",
    "    forwardpass = {}\n",
    "    forwardpass['Z1'] = np.matmul(X, params['W1']) + params['b1']\n",
    "    forwardpass['A'] = activation(forwardpass['Z1'])\n",
    "    forwardpass['Z2'] = np.matmul(forwardpass['A'], params['W2']) + params['b2']\n",
    "    forwardpass['Y_hat'] = softmax(forwardpass['Z2'])\n",
    "    return forwardpass\n",
    "\n",
    "# Define Cross entropy loss\n",
    "def CrossEntropyLoss(y, y_hat):\n",
    "    # y: true label (one-hot code)\n",
    "    # y_hat: predicted probability\n",
    "    # Batch Size: N\n",
    "    N = y.shape[0]\n",
    "    loss = -(1/N) * np.sum(y * np.log(y_hat + 1e-5))\n",
    "    return loss\n",
    "\n",
    "def L2_regularization(params, weight_decay):\n",
    "    # weight decay (lambda): small positive number\n",
    "    sum = 0\n",
    "    sum += np.sum(params['W1']**2)\n",
    "    sum += np.sum(params['b1']**2)\n",
    "    sum += np.sum(params['W2']**2)\n",
    "    sum += np.sum(params['b2']**2)\n",
    "    reg = (weight_decay/2)*sum\n",
    "    return reg\n",
    "\n",
    "\n",
    "# BackPropagation\n",
    "def backward(X, y, forwardpass, params, activation):\n",
    "    '''\n",
    "    grads['Z2']: (batch, OUTPUT_DIM)\n",
    "    grads['W2']: (HIDDEN_DIM, OUTPUT_DIM)\n",
    "    grads['b2']: (1, OUTPUT_DIM)\n",
    "    grads['A'] : (batch, HIDDEN_DIM)\n",
    "    grads['Z1']: (batch, HIDDEN_DIM)\n",
    "    grads['W1']: (INPUT_DIM, HIDDEN_DIM)\n",
    "    grads['b1']: (1, HIDDEN_DIM)\n",
    "    X :          (batch, INPUT_DIM)\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    grads={}\n",
    "    grads['Z2'] = (1/N) * (forwardpass['Y_hat'] - y)\n",
    "    grads['W2'] = np.sum(np.matmul( forwardpass['A'].reshape(N,-1,1), grads['Z2'].reshape(N,1,-1)), \n",
    "                         axis=0, keepdims=False)\n",
    "    grads['b2'] = np.sum(grads['Z2'], axis=0, keepdims=True)\n",
    "    grads['A']  = np.matmul(grads['Z2'], params['W2'].T)\n",
    "\n",
    "    if activation == sigmoid:\n",
    "        grads['Z1'] = grads['A'] * forwardpass['A'] * (1-forwardpass['A'])\n",
    "\n",
    "    ######################################################################################\n",
    "    ############################### COMPLETE THE LINE  ###################################   \n",
    "    elif activation == relu:\n",
    "        grads['Z1'] =\n",
    "\n",
    "\n",
    "    ######################################################################################\n",
    "    \n",
    "    grads['W1'] = np.sum( np.matmul(X.reshape(N,-1,1), grads['Z1'].reshape(N,1,-1)), axis=0, keepdims=False)\n",
    "    grads['b1'] = np.sum( grads['Z1'], axis=0, keepdims=True)\n",
    "\n",
    "    return grads\n",
    "\n",
    "# Gradient Descent\n",
    "def update_params(params, grads, eta, weight_decay):\n",
    "    params['W1'] -= eta * ( grads['W1'] + weight_decay*params['W1'] )\n",
    "    params['b1'] -= eta * ( grads['b1'] + weight_decay*params['b1'] )\n",
    "    params['W2'] -= eta * ( grads['W2'] + weight_decay*params['W2'] )\n",
    "    params['b2'] -= eta * ( grads['b2'] + weight_decay*params['b2'] )\n",
    "    return params\n",
    "\n",
    "# Set Hyperparameters\n",
    "BATCH_SIZE=250\n",
    "EPOCHS=20\n",
    "LR = 0.01\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "\n",
    "## Used for validation check or evaluation check\n",
    "\n",
    "def predict(X, y, params, activation):\n",
    "    prob = forward_NN(X, params, activation)['Y_hat']\n",
    "    pred = np.argmax(prob, axis=1)\n",
    "    acc = np.sum(pred == y) / y.shape[0]\n",
    "    return pred, acc\n",
    "\n",
    "######################################################################################\n",
    "########################## COMPLETE THE INITIALIZATION  ##############################\n",
    "##########################     for RELU activation      ##############################\n",
    "ReLUParams = {\n",
    "    'W1': np.random.randn(INPUT_DIM, HIDDEN_DIM), \n",
    "    'b1': np.zeros((1, HIDDEN_DIM)),\n",
    "    'W2': np.random.randn(HIDDEN_DIM, OUTPUT_DIM),\n",
    "    'b2': np.zeros((1, OUTPUT_DIM))\n",
    "    }\n",
    "######################################################################################\n",
    "\n",
    "# Training Session\n",
    "losses_relu = np.zeros(EPOCHS)\n",
    "\n",
    "start = datetime.now()\n",
    "for epoch in range(EPOCHS):\n",
    "    idx = np.random.permutation(50000)\n",
    "    x_temp = x_train[idx]\n",
    "    y_temp = y_train_one_hot[idx]\n",
    "    \n",
    "    for batch in range(x_train.shape[0]//BATCH_SIZE):\n",
    "        X = x_temp[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        y = y_temp[batch*BATCH_SIZE:(batch+1)*BATCH_SIZE]\n",
    "        \n",
    "        forwardpass = forward_NN(X, ReLUParams, relu)\n",
    "        loss = CrossEntropyLoss(y, forwardpass['Y_hat']) + L2_regularization(ReLUParams, WEIGHT_DECAY)\n",
    "        grads = backward(X, y, forwardpass, ReLUParams, relu)\n",
    "        \n",
    "        ReLUParams = update_params(ReLUParams, grads, LR, WEIGHT_DECAY)\n",
    "    \n",
    "    losses_relu[epoch] = loss    \n",
    "    pred, valid_acc = predict(x_valid, y_valid, ReLUParams, relu)\n",
    "    \n",
    "    print('EPOCH %d Completed, Loss: %.3f, Validation Accuracy: %.4f' % (epoch+1, loss, valid_acc))\n",
    "\n",
    "end = datetime.now()\n",
    "print('Total time:', end-start)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(np.arange(1, EPOCHS+1), losses_relu)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()\n",
    "\n",
    "# Inference/Test Session\n",
    "pred, test_acc = predict(x_test, y_test, ReLUParams, relu)\n",
    "print('ReLU Test Accuracy:', test_acc*100,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
