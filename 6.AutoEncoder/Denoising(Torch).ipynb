{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary as summary_\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Using Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_factor = 0.3\n",
    "EPOCH = 30\n",
    "BATCH_SIZE = 100\n",
    "NUM_WORKERS = 2\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img):\n",
    "    noise = torch.randn(img.size()) * noise_factor\n",
    "    noisy_img = img + noise\n",
    "    return noisy_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./.data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 26574796.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./.data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 14451884.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./.data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 7814575.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./.data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fashion MNIST 데이터셋 불러오기\n",
    "trainset = datasets.MNIST(\n",
    "    root      = './.data/', \n",
    "    train     = True,\n",
    "    download  = True,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    root      = './.data/', \n",
    "    train     = False,\n",
    "    download  = True,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = trainset,\n",
    "    batch_size  = BATCH_SIZE,\n",
    "    shuffle     = True,\n",
    "    num_workers = NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = testset,\n",
    "    batch_size  = BATCH_SIZE,\n",
    "    shuffle     = False,\n",
    "    num_workers = NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오토인코더 모듈 정의\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "\n",
    "        self.encoder = nn.Sequential( # nn.Sequential을 사용해 encoder와 decoder 두 모듈로 묶어줍니다.\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, padding_mode='zeros'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(1, -1),\n",
    "            nn.Linear(32*7*7, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,3)\n",
    "\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32*7*7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1,(32,7,7)),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()       # 픽셀당 0과 1 사이로 값을 출력하는 sigmoid()함수를 추가합니다.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x) # encoder는 encoded라는 latent vector를 만들고\n",
    "        decoded = self.decoder(encoded) # decoder를 통해 decoded라는 복원이미지를 만듭니다.\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate) \n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [100, 16, 28, 28]             160\n",
      "              ReLU-2          [100, 16, 28, 28]               0\n",
      "         MaxPool2d-3          [100, 16, 14, 14]               0\n",
      "            Conv2d-4          [100, 32, 14, 14]           4,640\n",
      "              ReLU-5          [100, 32, 14, 14]               0\n",
      "         MaxPool2d-6            [100, 32, 7, 7]               0\n",
      "           Flatten-7                [100, 1568]               0\n",
      "            Linear-8                  [100, 64]         100,416\n",
      "              ReLU-9                  [100, 64]               0\n",
      "           Linear-10                   [100, 3]             195\n",
      "================================================================\n",
      "Total params: 105,411\n",
      "Trainable params: 105,411\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.30\n",
      "Forward/backward pass size (MB): 33.60\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 34.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary_(autoencoder.encoder, (1,28,28), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [100, 64]             256\n",
      "              ReLU-2                  [100, 64]               0\n",
      "            Linear-3                [100, 1568]         101,920\n",
      "              ReLU-4                [100, 1568]               0\n",
      "         Unflatten-5            [100, 32, 7, 7]               0\n",
      "   ConvTranspose2d-6          [100, 16, 14, 14]           2,064\n",
      "              ReLU-7          [100, 16, 14, 14]               0\n",
      "   ConvTranspose2d-8           [100, 1, 28, 28]              65\n",
      "           Sigmoid-9           [100, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 104,305\n",
      "Trainable params: 104,305\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 9.67\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 10.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary_(autoencoder.decoder, (3,), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 이미지를 시각화 하기 (첫번째 열)\n",
    "view_data = testset.data[:10].view(-1, 1, 28, 28)\n",
    "view_data = view_data.type(torch.FloatTensor)/255.\n",
    "#픽셀의 색상값이 0~255이므로 모델이 인식하는 0부터 1사이의 값으로 만들기 위해 255로 나눠줍니다.\n",
    "view_data = add_noise(view_data)\n",
    "view_data = torch.clamp(view_data, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습하기 위한 함수\n",
    "def train(autoencoder, train_loader):\n",
    "    autoencoder.train()\n",
    "    total_loss=0\n",
    "    for step, (x, label) in enumerate(train_loader):\n",
    "        x = x.view(-1, 1, 28, 28).to(DEVICE)\n",
    "        x_noised = add_noise(x)\n",
    "        x_noised = torch.clamp(x_noised, min=0.0, max=1.0)\n",
    "        y = x.view(-1, 1, 28, 28).to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        encoded, decoded = autoencoder(x_noised)\n",
    "\n",
    "        loss = criterion(decoded, y)\n",
    "        total_loss = total_loss + loss\n",
    "        optimizer.zero_grad() #기울기에 대한 정보를 초기화합니다.\n",
    "        loss.backward() # 기울기를 구합니다.\n",
    "        optimizer.step() #최적화를 진행합니다.\n",
    "\n",
    "    total_loss = total_loss.item()\n",
    "    print(\"total train loss:\" + str(total_loss))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(autoencoder, test_loader):\n",
    "    autoencoder.eval()\n",
    "    total_loss=0\n",
    "    for step, (x, label) in enumerate(test_loader):\n",
    "        x = x.view(-1, 1, 28, 28).to(DEVICE)\n",
    "        x_noised = add_noise(x)\n",
    "        x_noised = torch.clamp(x_noised, min=0.0, max=1.0)\n",
    "        y = x.view(-1, 1, 28, 28).to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        encoded, decoded = autoencoder(x_noised)\n",
    "\n",
    "        loss = criterion(decoded, y) # decoded와 원본이미지(y) 사이의 평균제곱오차를 구합니다\n",
    "        total_loss = total_loss + loss\n",
    "\n",
    "\n",
    "    total_loss = total_loss.item()\n",
    "    print(\"total test loss:\" + str(total_loss))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train loss:34.483097076416016\n",
      "total test loss:4.491387367248535\n"
     ]
    }
   ],
   "source": [
    "train_losses=[]\n",
    "test_losses=[]\n",
    "#학습하기\n",
    "for epoch in range(1, EPOCH+1):\n",
    "    loss_epoch = train(autoencoder, train_loader)\n",
    "    train_losses.append(loss_epoch)\n",
    "\n",
    "    loss_epoch = test(autoencoder, test_loader)\n",
    "    test_losses.append(loss_epoch)\n",
    "\n",
    "    test_x = view_data.to(DEVICE)\n",
    "    _, decoded_data = autoencoder(test_x)\n",
    "\n",
    "    # 원본과 디코딩 결과 비교해보기\n",
    "    f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
    "    print(\"[Epoch {}]\".format(epoch))\n",
    "    for i in range(10):\n",
    "        img = np.reshape(view_data.data.numpy()[i],(28, 28)) #파이토치 텐서를 넘파이로 변환합니다.\n",
    "        a[0][i].imshow(img, cmap='gray')\n",
    "        a[0][i].set_xticks(()); a[0][i].set_yticks(())\n",
    "\n",
    "    for i in range(10):\n",
    "        img = np.reshape(decoded_data.to(\"cpu\").data.numpy()[i], (28, 28)) \n",
    "        # CUDA를 사용하면 모델 출력값이 GPU에 남아있으므로 .to(\"cpu\") 함수로 일반메모리로 가져와 numpy행렬로 변환합니다.\n",
    "        # cpu를 사용할때에도 같은 코드를 사용해도 무방합니다.\n",
    "        a[1][i].imshow(img, cmap='gray')\n",
    "        a[1][i].set_xticks(()); a[1][i].set_yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1,EPOCH+1)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x, train_losses, 'g-')\n",
    "ax2.plot(x, test_losses, 'b-')\n",
    "ax1.set_ylabel('train_loss', color='g')\n",
    "ax2.set_ylabel('test_loss', color='b')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b45ccf6fd3c8427b2d1b5e880d896f46770117107f616a4b6b7de4d4387dfaf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
